{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft\n",
    "import matplotlib.pyplot as plt\n",
    "from math import atan, degrees, radians\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, kernel_size=7):\n",
    "        super(LSTMBinaryClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем сверточный слой\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_size, seq_len) -> (batch_size, seq_len, input_size)\n",
    "        x = self.conv(x)        # (batch_size, hidden_size, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)    # Берем только последний выход LSTM\n",
    "        out = self.sigmoid(out)         # Применяем сигмоиду для получения вероятности\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(metadata_filepath, sample_freq, N_start=0, N_end=10, only_health=False):\n",
    "    \"\"\"Загрузка данных из файлов.\\n\n",
    "    Работает только при наличии директории 'records',\\n\n",
    "    в которой пять папок по 5000 файлов записей\"\"\"\n",
    "    \n",
    "    # Загружаем метаданные и выбираем срезом нужные\n",
    "    metadata_full = pd.read_csv('metadata.csv')\n",
    "    metadata = metadata_full.iloc[N_start:N_end]\n",
    "\n",
    "    # Убираем лишние столбцы, приводим к типу int32\n",
    "    metadata = metadata.drop(columns=['Date', 'Patient_ID'])\n",
    "    metadata['Age'].astype(np.int32)\n",
    "    metadata['N'].astype(np.int32)\n",
    "\n",
    "\n",
    "    if only_health:\n",
    "        # Берём только здоровых людей\n",
    "        health_metadata = metadata.loc[metadata['AHA_Code'] == '1']\n",
    "        id_metadata = health_metadata['ECG_ID'].values\n",
    "        labels = health_metadata['AHA_Code'].values\n",
    "    else:\n",
    "        # Берём всех\n",
    "        id_metadata = metadata['ECG_ID'].values\n",
    "        labels = metadata['AHA_Code'].values\n",
    "\n",
    "    signals = []\n",
    "\n",
    "    for i in id_metadata:\n",
    "        # Из Axxxxx оставляем численную часть\n",
    "        number = int(i[1:])\n",
    "\n",
    "        # В каждой папке 5000 файлов (кроме пятой), поэтому,\n",
    "        # чтобы узнать номер папки, в которой запись, делим на 5000\n",
    "        record_num = (number - 1) // 5000\n",
    "        if record_num > 4:\n",
    "            record_num = 4\n",
    "        record_num = record_num + 1\n",
    "        \n",
    "        # Считываем файл.\n",
    "        with h5py.File(f'records/record{record_num}/{i}.h5', 'r') as f:\n",
    "            signals.append(f['ecg'][()])\n",
    "    \n",
    "    return signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_transform_tensor(signals, N=0, max_len_signal=5000):\n",
    "    \"\"\"Транформирует сигналы в тензор.\\n\n",
    "    N - номер отведения в соответствии с массивом:\\n\n",
    "    ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\"\"\"\n",
    "\n",
    "    # Ограничиваем максимальную длину до единого значения для использования в батче.\n",
    "    # Дописывать нули в конце для одной длины - сомнительная идея для LSTM.\n",
    "\n",
    "    result_reshape = np.reshape([signal[N][0:max_len_signal] for signal in signals], (1, max_len_signal * len(signals)))\n",
    "\n",
    "    return torch.tensor(result_reshape, dtype=torch.float32).reshape((len(signals), max_len_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_transform_tensor(labels, all_diagnoses=False):\n",
    "    \"\"\"Транформирует лейблы в тензор.\\n\n",
    "    Сейчас только в режиме 'Норма-Не норма'.\"\"\"\n",
    "\n",
    "    # TODO: сделать разбиение лейблов на первичное и вторичное заключение врача\n",
    "\n",
    "    result_labels = []\n",
    "\n",
    "    if all_diagnoses:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Норма - Не норма\n",
    "        for label in labels:\n",
    "            if label == '1':\n",
    "                result_labels.append(0)\n",
    "            else:\n",
    "                result_labels.append(1)\n",
    "\n",
    "    result_labels = torch.tensor(result_labels, dtype=torch.float32).reshape((len(result_labels), 1))\n",
    "\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filepath = 'metadata.csv'\n",
    "sample_freq = 500\n",
    "\n",
    "signals, labels = load_data(metadata_filepath, sample_freq, N_end=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_transformed = label_transform_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_0_transformed = signal_transform_tensor(signals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Backward pass и оптимизация\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Разделение данных на вход (X) и метки (y)\n",
    "X = signals_0_transformed.unsqueeze(-1)  # (batch_size, seq_len, input_size)\n",
    "y = labels_transformed\n",
    "\n",
    "# Создание TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Создание DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Параметры модели\n",
    "input_size = 1  # Один признак на каждый временной шаг\n",
    "hidden_size = 50\n",
    "output_size = 1  # Один выход для бинарной классификации\n",
    "num_layers = 2\n",
    "\n",
    "model_0 = LSTMBinaryClassifier(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 3. Определение функции потерь и оптимизатора\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Обучение модели\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_0.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_0(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. Оценка модели\n",
    "model_0.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model_0(batch_X)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_y.tolist(), label='True Labels', marker='o', linestyle='None')\n",
    "plt.plot(predicted.tolist(), label='Predicted Labels', marker='x', linestyle='None')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_0_model_iteration(N, save_weights_name, _channel=0, num_epochs=10, lr=0.01, momentum=0.9, weight_decay=0.0001):\n",
    "    metadata_filepath = 'metadata.csv'\n",
    "    sample_freq = 500\n",
    "\n",
    "    # Загружаем данные\n",
    "    signals, labels = load_data(metadata_filepath, sample_freq, N_end=N)\n",
    "\n",
    "    # Трансформируем данные для обучения из всех отведений (отдельно, потому что используем ансамбль LSTM)\n",
    "    signals_X_transformed = []\n",
    "    for i in range(12):\n",
    "        signals_X_transformed.append(signal_transform_tensor(signals, i))\n",
    "\n",
    "    labels_transformed = label_transform_tensor(labels)\n",
    "\n",
    "    # Разделение данных на вход (X) и метки (y)\n",
    "    X = signals_X_transformed[_channel].unsqueeze(-1)  # (batch_size, seq_len, input_size)\n",
    "    y = labels_transformed\n",
    "    return X\n",
    "\n",
    "    # Создание TensorDataset\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    # Разделение на обучающую и тестовую выборки\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Создание DataLoader\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Параметры модели\n",
    "    input_size = 1  # Один признак на каждый временной шаг\n",
    "    hidden_size = 50\n",
    "    output_size = 1  # Один выход для бинарной классификации\n",
    "    num_layers = 1\n",
    "\n",
    "    model_0 = LSTMBinaryClassifier(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "    # Определение функции потерь и оптимизатора\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    # optimizer = torch.optim.Adam(model_0.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.SGD(model_0.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Обучение модели\n",
    "    for epoch in range(num_epochs):\n",
    "        model_0.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model_0(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass и оптимизация\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Оценка модели\n",
    "    model_0.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model_0(batch_X)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Визуализация результатов\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(batch_y.tolist(), label='True Labels', marker='o', linestyle='None')\n",
    "    plt.plot(predicted.tolist(), label='Predicted Labels', marker='x', linestyle='None')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model_0.state_dict(), save_weights_name)\n",
    "\n",
    "    return model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './weights/0/2_SGD_10_0.01_0.9_0.0001.pth'\n",
    "# save_path = './weights/11/1.pth'\n",
    "\n",
    "sss = pipeline_0_model_iteration(512, save_path, _channel=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_X_transformed = []\n",
    "for i in range(12):\n",
    "    signals_X_transformed.append(signal_transform_tensor(signals, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(signals_X_transformed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLYALSTMBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, kernel_size=7):\n",
    "        super(BLYALSTMBinaryClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем сверточный слой\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_size, seq_len) -> (batch_size, seq_len, input_size)\n",
    "        x = self.conv(x)        # (batch_size, hidden_size, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)    # Берем только последний выход LSTM\n",
    "        out = self.sigmoid(out)         # Применяем сигмоиду для получения вероятности\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 10, 5)  # (batch_size, sequence_length, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2118,  1.2048,  2.4394, -1.3664,  0.0888],\n",
      "        [-0.8543,  0.0704,  1.5579,  0.7673,  0.6482],\n",
      "        [-0.2627,  0.4800,  0.6952, -0.6255,  0.8288],\n",
      "        [-0.6129,  0.3736,  0.6579,  0.0688,  1.4819],\n",
      "        [ 0.7184,  0.9687,  2.1419, -2.5235,  2.2264],\n",
      "        [ 0.0337, -0.9275,  0.1601,  0.2660,  0.1792],\n",
      "        [ 1.5888, -0.9229, -0.3337,  1.4416, -0.4635],\n",
      "        [ 0.5073,  0.3054,  1.6946,  1.2548, -1.2654],\n",
      "        [-0.5431, -0.6895, -0.1759, -0.0500,  0.5812],\n",
      "        [ 0.7347,  0.0320,  1.2294, -0.9344, -0.2164]])\n"
     ]
    }
   ],
   "source": [
    "# modelka = BLYALSTMBinaryClassifier(5, 50, 1)\n",
    "# modelka.forward(x)\n",
    "\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_transform_tensor_12(signals, N=0, max_len_signal=5000):\n",
    "    \"\"\"Транформирует сигналы в тензор.\\n\n",
    "    N - номер отведения в соответствии с массивом:\\n\n",
    "    ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\"\"\"\n",
    "\n",
    "    # Ограничиваем максимальную длину до единого значения для использования в батче.\n",
    "    # Дописывать нули в конце для одной длины - сомнительная идея для LSTM.\n",
    "\n",
    "    results_reshape = []\n",
    "\n",
    "    for i in range(12):\n",
    "        results_reshape.append(np.reshape([signal[i][0:max_len_signal] for signal in signals], (1, max_len_signal * len(signals))))\n",
    "\n",
    "    result_reshape = np.concatenate(results_reshape)\n",
    "\n",
    "    return torch.tensor(result_reshape, dtype=torch.float32).reshape((12, len(signals), max_len_signal)).permute((1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all = signal_transform_tensor_12(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4957],\n",
       "        [0.5000],\n",
       "        [0.4901],\n",
       "        [0.5054],\n",
       "        [0.5051],\n",
       "        [0.4934],\n",
       "        [0.5064],\n",
       "        [0.5018],\n",
       "        [0.4982],\n",
       "        [0.5032],\n",
       "        [0.5080],\n",
       "        [0.4948],\n",
       "        [0.4605],\n",
       "        [0.5093],\n",
       "        [0.5041],\n",
       "        [0.5052],\n",
       "        [0.4915],\n",
       "        [0.4989],\n",
       "        [0.4956],\n",
       "        [0.4981],\n",
       "        [0.5300],\n",
       "        [0.5060],\n",
       "        [0.5072],\n",
       "        [0.5011],\n",
       "        [0.5110],\n",
       "        [0.4963],\n",
       "        [0.5046],\n",
       "        [0.5134],\n",
       "        [0.5070],\n",
       "        [0.4984],\n",
       "        [0.5020],\n",
       "        [0.5002],\n",
       "        [0.5066],\n",
       "        [0.4936],\n",
       "        [0.4917],\n",
       "        [0.5162],\n",
       "        [0.4964],\n",
       "        [0.5027],\n",
       "        [0.4904],\n",
       "        [0.4974],\n",
       "        [0.4924],\n",
       "        [0.4894],\n",
       "        [0.4985],\n",
       "        [0.5101],\n",
       "        [0.5027],\n",
       "        [0.5037],\n",
       "        [0.4960],\n",
       "        [0.5049],\n",
       "        [0.4949],\n",
       "        [0.5037],\n",
       "        [0.5021],\n",
       "        [0.4930],\n",
       "        [0.5144],\n",
       "        [0.4992],\n",
       "        [0.5098],\n",
       "        [0.5030],\n",
       "        [0.5006],\n",
       "        [0.4936],\n",
       "        [0.5036],\n",
       "        [0.5012],\n",
       "        [0.5069],\n",
       "        [0.4971],\n",
       "        [0.5032],\n",
       "        [0.4991],\n",
       "        [0.5057],\n",
       "        [0.5057],\n",
       "        [0.5098],\n",
       "        [0.5146],\n",
       "        [0.5035],\n",
       "        [0.5037],\n",
       "        [0.4991],\n",
       "        [0.5097],\n",
       "        [0.4952],\n",
       "        [0.4971],\n",
       "        [0.4967],\n",
       "        [0.5054],\n",
       "        [0.5006],\n",
       "        [0.5153],\n",
       "        [0.5065],\n",
       "        [0.4974],\n",
       "        [0.5008],\n",
       "        [0.4953],\n",
       "        [0.4998],\n",
       "        [0.5030],\n",
       "        [0.5031],\n",
       "        [0.5011],\n",
       "        [0.4976],\n",
       "        [0.4939],\n",
       "        [0.5102],\n",
       "        [0.4977],\n",
       "        [0.4961],\n",
       "        [0.5026],\n",
       "        [0.4988],\n",
       "        [0.4923],\n",
       "        [0.5073],\n",
       "        [0.4938],\n",
       "        [0.5108],\n",
       "        [0.5033],\n",
       "        [0.4987],\n",
       "        [0.4404],\n",
       "        [0.4996],\n",
       "        [0.4985],\n",
       "        [0.5063],\n",
       "        [0.5096],\n",
       "        [0.4919],\n",
       "        [0.4973],\n",
       "        [0.5039],\n",
       "        [0.4983],\n",
       "        [0.4974],\n",
       "        [0.5035],\n",
       "        [0.4956],\n",
       "        [0.5095],\n",
       "        [0.5072],\n",
       "        [0.5077],\n",
       "        [0.4997],\n",
       "        [0.4938],\n",
       "        [0.5016],\n",
       "        [0.4862],\n",
       "        [0.4947],\n",
       "        [0.4946],\n",
       "        [0.4997],\n",
       "        [0.5120],\n",
       "        [0.5040],\n",
       "        [0.5008],\n",
       "        [0.5056],\n",
       "        [0.4971],\n",
       "        [0.5047],\n",
       "        [0.4908],\n",
       "        [0.5065],\n",
       "        [0.4920],\n",
       "        [0.5107],\n",
       "        [0.4888],\n",
       "        [0.5036],\n",
       "        [0.5030],\n",
       "        [0.5032],\n",
       "        [0.4979],\n",
       "        [0.4797],\n",
       "        [0.4991],\n",
       "        [0.5003],\n",
       "        [0.4935],\n",
       "        [0.5079],\n",
       "        [0.5096],\n",
       "        [0.4969],\n",
       "        [0.4860],\n",
       "        [0.4937],\n",
       "        [0.4614],\n",
       "        [0.4716],\n",
       "        [0.5013],\n",
       "        [0.5115],\n",
       "        [0.5024],\n",
       "        [0.4859],\n",
       "        [0.5066],\n",
       "        [0.4932],\n",
       "        [0.5104],\n",
       "        [0.4918],\n",
       "        [0.5042],\n",
       "        [0.5144],\n",
       "        [0.5085],\n",
       "        [0.5096],\n",
       "        [0.4953],\n",
       "        [0.5084],\n",
       "        [0.5084],\n",
       "        [0.4999],\n",
       "        [0.5003],\n",
       "        [0.5073],\n",
       "        [0.4955],\n",
       "        [0.5086],\n",
       "        [0.5001],\n",
       "        [0.5083],\n",
       "        [0.5095],\n",
       "        [0.4997],\n",
       "        [0.5118],\n",
       "        [0.5002],\n",
       "        [0.5028],\n",
       "        [0.4773],\n",
       "        [0.4945],\n",
       "        [0.5088],\n",
       "        [0.4998],\n",
       "        [0.5053],\n",
       "        [0.5070],\n",
       "        [0.4911],\n",
       "        [0.5041],\n",
       "        [0.5093],\n",
       "        [0.5015],\n",
       "        [0.5037],\n",
       "        [0.4925],\n",
       "        [0.4954],\n",
       "        [0.4669],\n",
       "        [0.5052],\n",
       "        [0.4993],\n",
       "        [0.5027],\n",
       "        [0.5021],\n",
       "        [0.4960],\n",
       "        [0.4965],\n",
       "        [0.5093],\n",
       "        [0.5014],\n",
       "        [0.5007],\n",
       "        [0.5071],\n",
       "        [0.5002],\n",
       "        [0.5044],\n",
       "        [0.5052],\n",
       "        [0.4991],\n",
       "        [0.4995],\n",
       "        [0.5010],\n",
       "        [0.4968],\n",
       "        [0.5035],\n",
       "        [0.5060],\n",
       "        [0.4997],\n",
       "        [0.4931],\n",
       "        [0.5021],\n",
       "        [0.5082],\n",
       "        [0.5020],\n",
       "        [0.5037],\n",
       "        [0.4998],\n",
       "        [0.5047],\n",
       "        [0.5061],\n",
       "        [0.5083],\n",
       "        [0.4976],\n",
       "        [0.5129],\n",
       "        [0.4557],\n",
       "        [0.5104],\n",
       "        [0.5017],\n",
       "        [0.5043],\n",
       "        [0.4976],\n",
       "        [0.4549],\n",
       "        [0.5123],\n",
       "        [0.5050],\n",
       "        [0.5029],\n",
       "        [0.4981],\n",
       "        [0.4994],\n",
       "        [0.4985],\n",
       "        [0.5079],\n",
       "        [0.4977],\n",
       "        [0.5052],\n",
       "        [0.5055],\n",
       "        [0.4971],\n",
       "        [0.4985],\n",
       "        [0.5015],\n",
       "        [0.5006],\n",
       "        [0.4940],\n",
       "        [0.5048],\n",
       "        [0.4975],\n",
       "        [0.5151],\n",
       "        [0.4963],\n",
       "        [0.5006],\n",
       "        [0.5035],\n",
       "        [0.5080],\n",
       "        [0.5014],\n",
       "        [0.4923],\n",
       "        [0.4869],\n",
       "        [0.5067],\n",
       "        [0.5030],\n",
       "        [0.4888],\n",
       "        [0.5015],\n",
       "        [0.5008],\n",
       "        [0.4987]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelka = BLYALSTMBinaryClassifier(12, 50, 1)\n",
    "modelka.forward(res_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
